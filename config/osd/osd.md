| Name | Desc | Level | Type | non-Daemon Default | Daemon Default | Min | Max | Valid Values | verbatim | See also | Flags | Services | Validator | Long Desc | Tags |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| <span id="SP_osd_agent_delay_time">osd_agent_delay_time</span> |  how long agent should sleep if it has no work to do | Advanced | Float | 5 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_agent_hist_halflife">osd_agent_hist_halflife</span> |  halflife of agent atime and temp histograms | Advanced | Int | 1000 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_agent_max_low_ops">osd_agent_max_low_ops</span> |  maximum concurrent low-priority tiering operations for tiering agent | Advanced | Int | 2 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_agent_max_ops">osd_agent_max_ops</span> |  maximum concurrent tiering operations for tiering agent | Advanced | Int | 4 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_agent_min_evict_effort">osd_agent_min_evict_effort</span> |  minimum effort to expend evicting clean objects | Advanced | Float | 0.1 |  | 0 | 0.99 |  |  |  |  |  |  |  |  |
| <span id="SP_osd_agent_quantize_effort">osd_agent_quantize_effort</span> |  size of quantize unit for eviction effort | Advanced | Float | 0.1 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_agent_slop">osd_agent_slop</span> |  slop factor to avoid switching tiering flush and eviction mode | Advanced | Float | 0.02 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_aggregated_slow_ops_logging">osd_aggregated_slow_ops_logging</span> |  Allow OSD daemon to send an aggregated slow ops to the cluster log | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_allow_recovery_below_min_size">osd_allow_recovery_below_min_size</span> |  allow replicated pools to recover with < min_size active members | Dev | Bool | True |  |  |  |  |  |  |  | osd |  |  |  |
| <span id="SP_osd_backfill_retry_interval">osd_backfill_retry_interval</span> |  how frequently to retry backfill reservations after being denied (e.g., due to a full OSD) | Advanced | Float | 30 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_backfill_scan_max">osd_backfill_scan_max</span> |   | Advanced | Int | 512 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_backfill_scan_min">osd_backfill_scan_min</span> |   | Advanced | Int | 64 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_check_max_object_name_len_on_startup">osd_check_max_object_name_len_on_startup</span> |   | Dev | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_class_default_list">osd_class_default_list</span> |   | Advanced | Str | cephfs hello journal lock log numops otp rbd refcount rgw rgw_gc timeindex user version cas cmpomap queue 2pc_queue fifo |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_class_dir">osd_class_dir</span> |   | Advanced | Str | CMAKE_INSTALL_LIBDIR/rados-classes |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_class_load_list">osd_class_load_list</span> |   | Advanced | Str | cephfs hello journal lock log numops otp rbd refcount rgw rgw_gc timeindex user version cas cmpomap queue 2pc_queue fifo |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_class_update_on_start">osd_class_update_on_start</span> |  set OSD device class on startup | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_client_message_cap">osd_client_message_cap</span> |  maximum number of in-flight client requests | Advanced | Uint | 256 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_client_message_size_cap">osd_client_message_size_cap</span> |  maximum memory to devote to in-flight client requests | Advanced | Size | 500_M |  |  |  |  |  |  |  |  |  | If this value is exceeded, the OSD will not read any new client data off of the network until memory is freed. |  |
| <span id="SP_osd_compact_on_start">osd_compact_on_start</span> |  compact OSD's object store's OMAP on start | Advanced | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_copyfrom_max_chunk">osd_copyfrom_max_chunk</span> |   | Advanced | Size | 8_M |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_crush_initial_weight">osd_crush_initial_weight</span> |  if >= 0, initial CRUSH weight for newly created OSDs | Advanced | Float | -1 |  |  |  |  |  |  |  |  |  | If this value is negative, the size of the OSD in TiB is used. |  |
| <span id="SP_osd_crush_update_on_start">osd_crush_update_on_start</span> |  update OSD CRUSH location on startup | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_data">osd_data</span> |  path to OSD data | Advanced | Str | /var/lib/ceph/osd/$cluster-$id |  |  |  |  |  |  | NO_MON_UPDATE |  |  |  |  |
| <span id="SP_osd_debug_feed_pullee">osd_debug_feed_pullee</span> |  Feed a pullee, and force primary to pull a currently missing object from it | Dev | Int | -1 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_deep_scrub_interval">osd_deep_scrub_interval</span> |  Deep scrub each PG (i.e., verify data checksums) at least this often | Advanced | Float | 7_day |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_deep_scrub_keys">osd_deep_scrub_keys</span> |  Number of keys to read from an object at a time during deep scrub | Advanced | Int | 1024 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_deep_scrub_large_omap_object_key_threshold">osd_deep_scrub_large_omap_object_key_threshold</span> |  Warn when we encounter an object with more omap keys than this | Advanced | Uint | 200000 |  |  |  |  |  | [[osd_deep_scrub_large_omap_object_value_sum_threshold](./osd/osd.md#SP_osd_deep_scrub_large_omap_object_value_sum_threshold)] |  | ["osd", "mds"] |  |  |  |
| <span id="SP_osd_deep_scrub_large_omap_object_value_sum_threshold">osd_deep_scrub_large_omap_object_value_sum_threshold</span> |  Warn when we encounter an object with more omap key bytes than this | Advanced | Size | 1_G |  |  |  |  |  | [[osd_deep_scrub_large_omap_object_key_threshold](./osd/osd.md#SP_osd_deep_scrub_large_omap_object_key_threshold)] |  | osd |  |  |  |
| <span id="SP_osd_deep_scrub_randomize_ratio">osd_deep_scrub_randomize_ratio</span> |  Scrubs will randomly become deep scrubs at this rate (0.15 -> 15% of scrubs are deep) | Advanced | Float | 0.15 |  |  |  |  |  |  |  |  |  | This prevents a deep scrub 'stampede' by spreading deep scrubs so they are uniformly distributed over the week |  |
| <span id="SP_osd_deep_scrub_stride">osd_deep_scrub_stride</span> |  Number of bytes to read from an object at a time during deep scrub | Advanced | Size | 512_K |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_deep_scrub_update_digest_min_age">osd_deep_scrub_update_digest_min_age</span> |  Update overall object digest only if object was last modified longer ago than this | Advanced | Int | 2_hr |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_delete_sleep">osd_delete_sleep</span> |  Time in seconds to sleep before next removal transaction (overrides values below) | Advanced | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_delete_sleep_hdd">osd_delete_sleep_hdd</span> |  Time in seconds to sleep before next removal transaction for HDDs | Advanced | Float | 5 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_delete_sleep_hybrid">osd_delete_sleep_hybrid</span> |  Time in seconds to sleep before next removal transaction when OSD data is on HDD and OSD journal or WAL+DB is on SSD | Advanced | Float | 1 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_delete_sleep_ssd">osd_delete_sleep_ssd</span> |  Time in seconds to sleep before next removal transaction for SSDs | Advanced | Float | 1 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_find_best_info_ignore_history_les">osd_find_best_info_ignore_history_les</span> |  ignore last_epoch_started value when peering AND PROBABLY LOSE DATA | Dev | Bool | False |  |  |  |  |  |  |  |  |  | THIS IS AN EXTREMELY DANGEROUS OPTION THAT SHOULD ONLY BE USED AT THE DIRECTION OF A DEVELOPER.  It makes peering ignore the last_epoch_started value when peering, which can allow the OSD to believe an OSD has an authoritative view of a PG's contents even when it is in fact old and stale, typically leading to data loss (by believing a stale PG is up to date). |  |
| <span id="SP_osd_heartbeat_min_peers">osd_heartbeat_min_peers</span> |   | Advanced | Int | 10 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_inject_bad_map_crc_probability">osd_inject_bad_map_crc_probability</span> |   | Dev | Float | 0 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_inject_failure_on_pg_removal">osd_inject_failure_on_pg_removal</span> |   | Dev | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_journal">osd_journal</span> |  path to OSD journal (when FileStore backend is in use) | Advanced | Str | /var/lib/ceph/osd/$cluster-$id/journal |  |  |  |  |  |  | NO_MON_UPDATE |  |  |  |  |
| <span id="SP_osd_journal_flush_on_shutdown">osd_journal_flush_on_shutdown</span> |  flush FileStore journal contents during clean OSD shutdown | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_journal_size">osd_journal_size</span> |  size of FileStore journal (in MiB) | Advanced | Size | 5_K |  |  |  |  |  |  | CREATE |  |  |  |  |
| <span id="SP_osd_map_cache_size">osd_map_cache_size</span> |   | Advanced | Int | 50 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_map_share_max_epochs">osd_map_share_max_epochs</span> |   | Advanced | Int | 40 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_backfills">osd_max_backfills</span> |  Maximum number of concurrent local and remote backfills or recoveries per OSD | Advanced | Uint | 1 |  |  |  |  |  |  | RUNTIME |  |  | There can be osd_max_backfills local reservations AND the same remote reservations per OSD. So a value of 1 lets this OSD participate as 1 PG primary in recovery and 1 shard of another recovering PG. |  |
| <span id="SP_osd_max_markdown_count">osd_max_markdown_count</span> |   | Advanced | Int | 5 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_markdown_period">osd_max_markdown_period</span> |   | Advanced | Int | 10_min |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_pgls">osd_max_pgls</span> |  maximum number of results when listing objects in a pool | Advanced | Uint | 1_K |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_push_cost">osd_max_push_cost</span> |   | Advanced | Size | 8_M |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_push_objects">osd_max_push_objects</span> |   | Advanced | Uint | 10 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_scrubs">osd_max_scrubs</span> |  Maximum concurrent scrubs on a single OSD | Advanced | Int | 1 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_max_write_size">osd_max_write_size</span> |  Maximum size of a RADOS write operation in megabytes | Advanced | Size | 90 |  | 4 |  |  |  |  |  |  |  | This setting prevents clients from doing very large writes to RADOS.  If you set this to a value below what clients expect, they will receive an error when attempting to write to the cluster. |  |
| <span id="SP_osd_mclock_cost_per_byte_usec">osd_mclock_cost_per_byte_usec</span> |  Cost per byte in microseconds to consider per OSD (overrides _ssd and _hdd if non-zero) | Dev | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the cost per byte to consider in microseconds per OSD. This is considered by the mclock scheduler to set an additional cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_cost_per_byte_usec_hdd">osd_mclock_cost_per_byte_usec_hdd</span> |  Cost per byte in microseconds to consider per OSD (for rotational media) | Dev | Float | 5.2 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the cost per byte to consider in microseconds per OSD for rotational device type. This is considered by the mclock_scheduler to set an additional cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_cost_per_byte_usec_ssd">osd_mclock_cost_per_byte_usec_ssd</span> |  Cost per byte in microseconds to consider per OSD (for solid state media) | Dev | Float | 0.011 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the cost per byte to consider in microseconds per OSD for solid state device type. This is considered by the mclock_scheduler to set an additional cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_cost_per_io_usec">osd_mclock_cost_per_io_usec</span> |  Cost per IO in microseconds to consider per OSD (overrides _ssd and _hdd if non-zero) | Dev | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the cost factor to consider in usec per OSD. This is considered by the mclock scheduler to set an additional cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_cost_per_io_usec_hdd">osd_mclock_cost_per_io_usec_hdd</span> |  Cost per IO in microseconds to consider per OSD (for rotational media) | Dev | Float | 25000 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the cost factor to consider in usec per OSD for rotational device type. This is considered by the mclock_scheduler to set an additional cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_cost_per_io_usec_ssd">osd_mclock_cost_per_io_usec_ssd</span> |  Cost per IO in microseconds to consider per OSD (for solid state media) | Dev | Float | 50 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the cost factor to consider in usec per OSD for solid state device type. This is considered by the mclock_scheduler to set an additional cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_force_run_benchmark_on_init">osd_mclock_force_run_benchmark_on_init</span> |  Force run the OSD benchmark on OSD initialization/boot-up | Advanced | Bool | False |  |  |  |  |  | [[osd_mclock_max_capacity_iops_hdd](./osd/osd.md#SP_osd_mclock_max_capacity_iops_hdd), [osd_mclock_max_capacity_iops_ssd](./osd/osd.md#SP_osd_mclock_max_capacity_iops_ssd)] | STARTUP |  |  | This option specifies whether the OSD benchmark must be run during the OSD boot-up sequence even if historical data about the OSD iops capacity is available in the MON config store. Enable this to refresh the OSD iops capacity if the underlying device's performance characteristics have changed significantly. Only considered for osd_op_queue = mclock_scheduler. |  |
| <span id="SP_osd_mclock_max_capacity_iops_hdd">osd_mclock_max_capacity_iops_hdd</span> |  Max IOPs capacity (at 4KiB block size) to consider per OSD (for rotational media) | Basic | Float | 315 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the max OSD capacity in iops per OSD. Helps in QoS calculations when enabling a dmclock profile. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_max_capacity_iops_ssd">osd_mclock_max_capacity_iops_ssd</span> |  Max IOPs capacity (at 4KiB block size) to consider per OSD (for solid state media) | Basic | Float | 21500 |  |  |  |  |  |  | RUNTIME |  |  | This option specifies the max OSD capacity in iops per OSD. Helps in QoS calculations when enabling a dmclock profile. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_profile">osd_mclock_profile</span> |  Which mclock profile to use | Advanced | Str | high_client_ops |  |  |  | ["balanced", "high_recovery_ops", "high_client_ops", "custom"] |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] | RUNTIME |  |  | This option specifies the mclock profile to enable - one among the set of built-in profiles or a custom profile. Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_anticipation_timeout">osd_mclock_scheduler_anticipation_timeout</span> |  mclock anticipation timeout in seconds | Advanced | Float | 0 |  |  |  |  |  |  |  |  |  | the amount of time that mclock waits until the unused resource is forfeited |  |
| <span id="SP_osd_mclock_scheduler_background_best_effort_lim">osd_mclock_scheduler_background_best_effort_lim</span> |  IO limit for background best_effort over reservation | Advanced | Uint | 999999 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_background_best_effort_res">osd_mclock_scheduler_background_best_effort_res</span> |  IO proportion reserved for background best_effort (default) | Advanced | Uint | 1 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_background_best_effort_wgt">osd_mclock_scheduler_background_best_effort_wgt</span> |  IO share for each background best_effort over reservation | Advanced | Uint | 1 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_background_recovery_lim">osd_mclock_scheduler_background_recovery_lim</span> |  IO limit for background recovery over reservation | Advanced | Uint | 999999 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_background_recovery_res">osd_mclock_scheduler_background_recovery_res</span> |  IO proportion reserved for background recovery (default) | Advanced | Uint | 1 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_background_recovery_wgt">osd_mclock_scheduler_background_recovery_wgt</span> |  IO share for each background recovery over reservation | Advanced | Uint | 1 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_client_lim">osd_mclock_scheduler_client_lim</span> |  IO limit for each client (default) over reservation | Advanced | Uint | 999999 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_client_res">osd_mclock_scheduler_client_res</span> |  IO proportion reserved for each client (default) | Advanced | Uint | 1 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_scheduler_client_wgt">osd_mclock_scheduler_client_wgt</span> |  IO share for each client (default) over reservation | Advanced | Uint | 1 |  |  |  |  |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | Only considered for osd_op_queue = mclock_scheduler |  |
| <span id="SP_osd_mclock_skip_benchmark">osd_mclock_skip_benchmark</span> |  Skip the OSD benchmark on OSD initialization/boot-up | Dev | Bool | False |  |  |  |  |  | [[osd_mclock_max_capacity_iops_hdd](./osd/osd.md#SP_osd_mclock_max_capacity_iops_hdd), [osd_mclock_max_capacity_iops_ssd](./osd/osd.md#SP_osd_mclock_max_capacity_iops_ssd)] | RUNTIME |  |  | This option specifies whether the OSD benchmark must be skipped during the OSD boot-up sequence. Only considered for osd_op_queue = mclock_scheduler. |  |
| <span id="SP_osd_min_recovery_priority">osd_min_recovery_priority</span> |  Minimum priority below which recovery is not performed | Advanced | Int | 0 |  |  |  |  |  |  |  |  |  | The purpose here is to prevent the cluster from doing *any* lower priority work (e.g., rebalancing) below this threshold and focus solely on higher priority work (e.g., replicating degraded objects). |  |
| <span id="SP_osd_num_cache_shards">osd_num_cache_shards</span> |  The number of cache shards to use in the object store. | Advanced | Size | 32 |  |  |  |  |  |  | STARTUP |  |  |  |  |
| <span id="SP_osd_numa_auto_affinity">osd_numa_auto_affinity</span> |  automatically set affinity to numa node when storage and network match | Advanced | Bool | True |  |  |  |  |  |  | STARTUP |  |  |  |  |
| <span id="SP_osd_numa_node">osd_numa_node</span> |  set affinity to a numa node (-1 for none) | Advanced | Int | -1 |  |  |  |  |  | [[osd_numa_auto_affinity](./osd/osd.md#SP_osd_numa_auto_affinity)] | STARTUP |  |  |  |  |
| <span id="SP_osd_numa_prefer_iface">osd_numa_prefer_iface</span> |  prefer IP on network interface on same numa node as storage | Advanced | Bool | True |  |  |  |  |  | [[osd_numa_auto_affinity](./osd/osd.md#SP_osd_numa_auto_affinity)] | STARTUP |  |  |  |  |
| <span id="SP_osd_op_num_shards">osd_op_num_shards</span> |   | Advanced | Int | 0 |  |  |  |  |  |  | STARTUP |  |  |  |  |
| <span id="SP_osd_op_num_shards_hdd">osd_op_num_shards_hdd</span> |   | Advanced | Int | 5 |  |  |  |  |  | [[osd_op_num_shards](./osd/osd.md#SP_osd_op_num_shards)] | STARTUP |  |  |  |  |
| <span id="SP_osd_op_num_shards_ssd">osd_op_num_shards_ssd</span> |   | Advanced | Int | 8 |  |  |  |  |  | [[osd_op_num_shards](./osd/osd.md#SP_osd_op_num_shards)] | STARTUP |  |  |  |  |
| <span id="SP_osd_op_num_threads_per_shard">osd_op_num_threads_per_shard</span> |   | Advanced | Int | 0 |  |  |  |  |  |  | STARTUP |  |  |  |  |
| <span id="SP_osd_op_num_threads_per_shard_hdd">osd_op_num_threads_per_shard_hdd</span> |   | Advanced | Int | 1 |  |  |  |  |  | [[osd_op_num_threads_per_shard](./osd/osd.md#SP_osd_op_num_threads_per_shard)] | STARTUP |  |  |  |  |
| <span id="SP_osd_op_num_threads_per_shard_ssd">osd_op_num_threads_per_shard_ssd</span> |   | Advanced | Int | 2 |  |  |  |  |  | [[osd_op_num_threads_per_shard](./osd/osd.md#SP_osd_op_num_threads_per_shard)] | STARTUP |  |  |  |  |
| <span id="SP_osd_op_pq_max_tokens_per_priority">osd_op_pq_max_tokens_per_priority</span> |   | Advanced | Uint | 4_M |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_op_pq_min_cost">osd_op_pq_min_cost</span> |   | Advanced | Size | 64_K |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_op_queue">osd_op_queue</span> |  which operation priority queue algorithm to use | Advanced | Str | mclock_scheduler |  |  |  | ["wpq", "mclock_scheduler", "debug_random"] |  | [[osd_op_queue_cut_off](./osd/osd.md#SP_osd_op_queue_cut_off)] |  |  |  | which operation priority queue algorithm to use |  |
| <span id="SP_osd_op_queue_cut_off">osd_op_queue_cut_off</span> |  the threshold between high priority ops and low priority ops | Advanced | Str | high |  |  |  | ["low", "high", "debug_random"] |  | [[osd_op_queue](./osd/osd.md#SP_osd_op_queue)] |  |  |  | the threshold between high priority ops that use strict priority ordering and low priority ops that use a fairness algorithm that may or may not incorporate priority |  |
| <span id="SP_osd_op_thread_suicide_timeout">osd_op_thread_suicide_timeout</span> |   | Advanced | Int | 150 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_op_thread_timeout">osd_op_thread_timeout</span> |   | Advanced | Int | 15 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_open_classes_on_start">osd_open_classes_on_start</span> |   | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_os_flags">osd_os_flags</span> |  flags to skip filestore omap or journal initialization | Dev | Uint | 0 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_pg_epoch_max_lag_factor">osd_pg_epoch_max_lag_factor</span> |  Max multiple of the map cache that PGs can lag before we throttle map injest | Advanced | Float | 2 |  |  |  |  |  | [[osd_map_cache_size](./osd/osd.md#SP_osd_map_cache_size)] |  |  |  |  |  |
| <span id="SP_osd_push_per_object_cost">osd_push_per_object_cost</span> |   | Advanced | Size | 1000 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_read_ec_check_for_errors">osd_read_ec_check_for_errors</span> |   | Advanced | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recover_clone_overlap">osd_recover_clone_overlap</span> |   | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recover_clone_overlap_limit">osd_recover_clone_overlap_limit</span> |   | Advanced | Uint | 10 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_delay_start">osd_recovery_delay_start</span> |   | Advanced | Float | 0 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recovery_max_active">osd_recovery_max_active</span> |  Number of simultaneous active recovery operations per OSD (overrides _ssd and _hdd if non-zero) | Advanced | Uint | 0 |  |  |  |  |  | [[osd_recovery_max_active_hdd](./osd/osd.md#SP_osd_recovery_max_active_hdd), [osd_recovery_max_active_ssd](./osd/osd.md#SP_osd_recovery_max_active_ssd)] | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_max_active_hdd">osd_recovery_max_active_hdd</span> |  Number of simultaneous active recovery operations per OSD (for rotational devices) | Advanced | Uint | 3 |  |  |  |  |  | [[osd_recovery_max_active](./osd/osd.md#SP_osd_recovery_max_active), [osd_recovery_max_active_ssd](./osd/osd.md#SP_osd_recovery_max_active_ssd)] | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_max_active_ssd">osd_recovery_max_active_ssd</span> |  Number of simultaneous active recovery operations per OSD (for non-rotational solid state devices) | Advanced | Uint | 10 |  |  |  |  |  | [[osd_recovery_max_active](./osd/osd.md#SP_osd_recovery_max_active), [osd_recovery_max_active_hdd](./osd/osd.md#SP_osd_recovery_max_active_hdd)] | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_max_chunk">osd_recovery_max_chunk</span> |   | Advanced | Size | 8_M |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recovery_max_omap_entries_per_chunk">osd_recovery_max_omap_entries_per_chunk</span> |   | Advanced | Uint | 8096 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recovery_max_single_start">osd_recovery_max_single_start</span> |   | Advanced | Uint | 1 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recovery_retry_interval">osd_recovery_retry_interval</span> |  how frequently to retry recovery reservations after being denied (e.g., due to a full OSD) | Advanced | Float | 30 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_recovery_sleep">osd_recovery_sleep</span> |  Time in seconds to sleep before next recovery or backfill op | Advanced | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_sleep_hdd">osd_recovery_sleep_hdd</span> |  Time in seconds to sleep before next recovery or backfill op for HDDs | Advanced | Float | 0.1 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_sleep_hybrid">osd_recovery_sleep_hybrid</span> |  Time in seconds to sleep before next recovery or backfill op when data is on HDD and journal is on SSD | Advanced | Float | 0.025 |  |  |  |  |  | [[osd_recovery_sleep](./osd/osd.md#SP_osd_recovery_sleep)] | RUNTIME |  |  |  |  |
| <span id="SP_osd_recovery_sleep_ssd">osd_recovery_sleep_ssd</span> |  Time in seconds to sleep before next recovery or backfill op for SSDs | Advanced | Float | 0 |  |  |  |  |  | [[osd_recovery_sleep](./osd/osd.md#SP_osd_recovery_sleep)] | RUNTIME |  |  |  |  |
| <span id="SP_osd_repair_during_recovery">osd_repair_during_recovery</span> |  Allow requested repairing when PGs on the OSD are undergoing recovery | Advanced | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_scrub_auto_repair">osd_scrub_auto_repair</span> |  Automatically repair damaged objects detected during scrub | Advanced | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_scrub_auto_repair_num_errors">osd_scrub_auto_repair_num_errors</span> |  Maximum number of detected errors to automatically repair | Advanced | Uint | 5 |  |  |  |  |  | [[osd_scrub_auto_repair](./osd/osd.md#SP_osd_scrub_auto_repair)] |  |  |  |  |  |
| <span id="SP_osd_scrub_backoff_ratio">osd_scrub_backoff_ratio</span> |  Backoff ratio for scheduling scrubs | Dev | Float | 0.66 |  |  |  |  |  |  |  |  |  | This is the precentage of ticks that do NOT schedule scrubs, 66% means that 1 out of 3 ticks will schedule scrubs |  |
| <span id="SP_osd_scrub_begin_hour">osd_scrub_begin_hour</span> |  Restrict scrubbing to this hour of the day or later | Advanced | Int | 0 |  | 0 | 23 |  |  | [[osd_scrub_end_hour](./osd/osd.md#SP_osd_scrub_end_hour)] |  |  |  | Use osd_scrub_begin_hour=0 and osd_scrub_end_hour=0 for the entire day. |  |
| <span id="SP_osd_scrub_begin_week_day">osd_scrub_begin_week_day</span> |  Restrict scrubbing to this day of the week or later | Advanced | Int | 0 |  | 0 | 6 |  |  | [[osd_scrub_end_week_day](./osd/osd.md#SP_osd_scrub_end_week_day)] |  |  |  | 0 = Sunday, 1 = Monday, etc. Use osd_scrub_begin_week_day=0 osd_scrub_end_week_day=0 for the entire week. |  |
| <span id="SP_osd_scrub_chunk_max">osd_scrub_chunk_max</span> |  Maximum number of objects to scrub in a single chunk | Advanced | Int | 25 |  |  |  |  |  | [[osd_scrub_chunk_min](./osd/osd.md#SP_osd_scrub_chunk_min)] |  |  |  |  |  |
| <span id="SP_osd_scrub_chunk_min">osd_scrub_chunk_min</span> |  Minimum number of objects to scrub in a single chunk | Advanced | Int | 5 |  |  |  |  |  | [[osd_scrub_chunk_max](./osd/osd.md#SP_osd_scrub_chunk_max)] |  |  |  |  |  |
| <span id="SP_osd_scrub_during_recovery">osd_scrub_during_recovery</span> |  Allow scrubbing when PGs on the OSD are undergoing recovery | Advanced | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_scrub_end_hour">osd_scrub_end_hour</span> |  Restrict scrubbing to hours of the day earlier than this | Advanced | Int | 0 |  | 0 | 23 |  |  | [[osd_scrub_begin_hour](./osd/osd.md#SP_osd_scrub_begin_hour)] |  |  |  | Use osd_scrub_begin_hour=0 and osd_scrub_end_hour=0 for the entire day. |  |
| <span id="SP_osd_scrub_end_week_day">osd_scrub_end_week_day</span> |  Restrict scrubbing to days of the week earlier than this | Advanced | Int | 0 |  | 0 | 6 |  |  | [[osd_scrub_begin_week_day](./osd/osd.md#SP_osd_scrub_begin_week_day)] |  |  |  | 0 = Sunday, 1 = Monday, etc. Use osd_scrub_begin_week_day=0 osd_scrub_end_week_day=0 for the entire week. |  |
| <span id="SP_osd_scrub_extended_sleep">osd_scrub_extended_sleep</span> |  Duration to inject a delay during scrubbing out of scrubbing hours | Advanced | Float | 0 |  |  |  |  |  | [[osd_scrub_begin_hour](./osd/osd.md#SP_osd_scrub_begin_hour), [osd_scrub_end_hour](./osd/osd.md#SP_osd_scrub_end_hour), [osd_scrub_begin_week_day](./osd/osd.md#SP_osd_scrub_begin_week_day), [osd_scrub_end_week_day](./osd/osd.md#SP_osd_scrub_end_week_day)] |  |  |  |  |  |
| <span id="SP_osd_scrub_interval_randomize_ratio">osd_scrub_interval_randomize_ratio</span> |  Ratio of scrub interval to randomly vary | Advanced | Float | 0.5 |  |  |  |  |  | [[osd_scrub_min_interval](./osd/osd.md#SP_osd_scrub_min_interval)] |  |  |  | This prevents a scrub 'stampede' by randomly varying the scrub intervals so that they are soon uniformly distributed over the week |  |
| <span id="SP_osd_scrub_invalid_stats">osd_scrub_invalid_stats</span> |   | Advanced | Bool | True |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_scrub_load_threshold">osd_scrub_load_threshold</span> |  Allow scrubbing when system load divided by number of CPUs is below this value | Advanced | Float | 0.5 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_scrub_max_interval">osd_scrub_max_interval</span> |  Scrub each PG no less often than this interval | Advanced | Float | 7_day |  |  |  |  |  | [[osd_scrub_min_interval](./osd/osd.md#SP_osd_scrub_min_interval)] |  |  |  |  |  |
| <span id="SP_osd_scrub_max_preemptions">osd_scrub_max_preemptions</span> |  Set the maximum number of times we will preempt a deep scrub due to a client operation before blocking client IO to complete the scrub | Advanced | Uint | 5 |  | 0 | 30 |  |  |  |  |  |  |  |  |
| <span id="SP_osd_scrub_min_interval">osd_scrub_min_interval</span> |  Scrub each PG no more often than this interval | Advanced | Float | 1_day |  |  |  |  |  | [[osd_scrub_max_interval](./osd/osd.md#SP_osd_scrub_max_interval)] |  |  |  |  |  |
| <span id="SP_osd_scrub_sleep">osd_scrub_sleep</span> |  Duration to inject a delay during scrubbing | Advanced | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_skip_data_digest">osd_skip_data_digest</span> |  Do not store full-object checksums if the backend (bluestore) does its own checksums.  Only usable with all BlueStore OSDs. | Dev | Bool | False |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_smart_report_timeout">osd_smart_report_timeout</span> |  Timeout (in seconds) for smartctl to run, default is set to 5 | Advanced | Uint | 5 |  |  |  |  |  |  |  |  |  |  |  |
| <span id="SP_osd_snap_trim_sleep">osd_snap_trim_sleep</span> |  Time in seconds to sleep before next snap trim (overrides values below) | Advanced | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_snap_trim_sleep_hdd">osd_snap_trim_sleep_hdd</span> |  Time in seconds to sleep before next snap trim for HDDs | Advanced | Float | 5 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_snap_trim_sleep_hybrid">osd_snap_trim_sleep_hybrid</span> |  Time in seconds to sleep before next snap trim when data is on HDD and journal is on SSD | Advanced | Float | 2 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_snap_trim_sleep_ssd">osd_snap_trim_sleep_ssd</span> |  Time in seconds to sleep before next snap trim for SSDs | Advanced | Float | 0 |  |  |  |  |  |  | RUNTIME |  |  |  |  |
| <span id="SP_osd_uuid">osd_uuid</span> |  uuid label for a new OSD | Advanced | Uuid |  |  |  |  |  |  |  | CREATE |  |  |  |  |
